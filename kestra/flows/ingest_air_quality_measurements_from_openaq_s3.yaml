######
### Years-wide ingestion. Ingests data from year <from_year> to year <to_year>.
### Used for high-volume history ingestion.
###
### Requires following KV:
###    GIT repo url - url of the project repository
###    AQ_API_KEY - OpenAQ api key
###    AQ_DATA_BUCKET_URL - url of the bucket in the following form: "gs://bucket-name/"
###    GCP_CREDS - Google Cloud Platform service account json
###    GCP_PROJECT - Google Cloud project
###    GCP_LOC - Google Cloud project location(example: "US")
###    BQ_DATASET - bigquery dataset, that will contain ingested data
######

id: ingest_air_quality_measurements_from_openaq_s3
namespace: company.team

concurrency:
  limit: 1

inputs:
  - id: countries
    type: ARRAY
    itemType: STRING
    defaults: ["Poland"]
  - id: from_year
    type: INT
    defaults: 2024
  - id: to_year
    type: INT
    defaults: 2025
#  - id: to_month
#    type: INT
#    defaults: 3
#  - id: from_month
#    type: INT
#    defaults: 1

  - id: ingest_to_gs
    type: BOOLEAN
    defaults: true
  - id: ingest_to_bq
    type: BOOLEAN
    defaults: true
  - id: do_topology_refresh
    type: BOOLEAN
    defaults: true

variables:
  years_list: "{{ range(inputs.from_year, inputs.to_year)}}"
  bq_measurements_table: "{{kv('GCP_PROJECT')}}.{{kv('BQ_DATASET')}}.m_measurements"
  bq_ext_table: "{{kv('GCP_PROJECT')}}.{{kv('BQ_DATASET')}}.tmp_ext"


pluginDefaults:
  - type: io.kestra.plugin.gcp
    values:
      serviceAccount: "{{kv('GCP_CREDS')}}"
      projectId: "{{kv('GCP_PROJECT')}}"
      location: "{{kv('GCP_LOC')}}"
      bucket: "{{kv('GCP_BUCKET')}}"

tasks:
  - id: set_labels
    type: io.kestra.plugin.core.execution.Labels
    labels:
      countries: "{{ inputs.countries | join(', ') }}"
      from: "{{ inputs.from_year }}" # "-{{ inputs.from_month }}"
      to: "{{ inputs.to_year }}" # "-{{inputs.to_month}}"
  - id: refresh_topology
    type: io.kestra.plugin.core.flow.Subflow
    runIf: "{{inputs.do_topology_refresh}}"
    namespace: "{{flow.namespace}}"
    flowId: ingest_air_quality_sensors_topology
    wait: true
    transmitFailed: true
  - id: sync
    type: io.kestra.plugin.git.SyncNamespaceFiles
    url: "{{ kv('GIT_REPO_URL') }}"
    namespace: "{{ flow.namespace }}"
    gitDirectory: "pipeline/aq_measurements_from_s3_lake_ingest"
  

  - id: for_each_country
    type: io.kestra.plugin.core.flow.ForEach
    values: "{{inputs.countries}}"
    tasks:
      - id: ingest_to_gcs
        type: io.kestra.plugin.scripts.python.Commands
        runIf: "{{inputs.ingest_to_gs}}"
        namespaceFiles:
          enabled: true
        taskRunner:
          type: io.kestra.plugin.core.runner.Process
        env:
          AQ_DATA_BUCKET_URL: "{{ kv('AQ_DATA_BUCKET_URL') }}"
          AQ_COUNTRY_NAME: "{{ taskrun.value }}"
          AQ_FROM_YEAR: "{{ inputs.from_year }}"
          #AQ_FROM_MO: "{{ inputs.from_month }}"
          AQ_TO_YEAR: "{{ inputs.to_year }}"
          #AQ_TO_MO: "{{ inputs.to_month }}"

          AWS_ACCESS_KEY_ID: "{{ kv('AWS_KEY') }}"
          AWS_SECRET_ACCESS_KEY: "{{ kv('AWS_SECRET') }}"
        beforeCommands:
          - pip install -r requirements.txt
        commands:
          - python ingest_v3.py
      - id: for_each_year
        runIf: "{{inputs.ingest_to_bq}}"
        type: io.kestra.plugin.core.flow.ForEach
        values: "{{ render(vars.years_list) }}"
        tasks:
          - id: create_ext_table
            type: "io.kestra.plugin.gcp.bigquery.Query"
            sql: |
              CREATE OR REPLACE EXTERNAL TABLE `{{render(vars.bq_ext_table)}}`(
                location_id INT64,
                sensors_id INT64,
                location STRING,
                datetime STRING,
                lat FLOAT64,
                lon FLOAT64,
                parameter STRING,
                units STRING,
                value FLOAT64
              ) OPTIONS(format="PARQUET", uris=["{{ kv('AQ_DATA_BUCKET_URL') }}aq/raw/measurements/{{ parents[0].taskrun.value | lower }}/{{ taskrun.value }}/*.parquet"])
          - id: create_target_resident_table
            type: "io.kestra.plugin.gcp.bigquery.Query"
            sql: |
              CREATE TABLE IF NOT EXISTS `{{render(vars.bq_measurements_table)}}`(
                location_id INT64,
                sensors_id INT64,
                location STRING,
                datetime DATETIME,
                lat FLOAT64,
                lon FLOAT64,
                parameter STRING,
                units STRING,
                value FLOAT64
              )
              PARTITION BY
                TIMESTAMP_TRUNC(datetime, MONTH)
              CLUSTER BY
                parameter;
          - id: from_ext_to_resident_table
            type: "io.kestra.plugin.gcp.bigquery.Query"
            sql: |
              MERGE `{{render(vars.bq_measurements_table)}}` T
              USING `{{render(vars.bq_ext_table)}}` S ON T.datetime = CAST(S.datetime AS DATETIME) AND T.parameter = S.parameter AND T.location_id = S.location_id AND T.sensors_id = S.sensors_id
              WHEN MATCHED THEN UPDATE SET value = s.value
              WHEN NOT MATCHED THEN INSERT (location_id, sensors_id, location, datetime, lat,lon, parameter,units,value) VALUES(location_id, sensors_id, location, CAST(datetime AS DATETIME), lat,lon, parameter,units,value);
                        